---
layout: post
title: "Research Diary from 2022/02/28"
author: Guanlin Li
tag: diary
---

- toc
{:toc}

### Tips

```bash
nohup train.sh > log/logfilename &  # run in background with std/err outputs in log file
```

#### Running `simpleTOD`

```bash
# no package named tensorboardX, spacy, nltk, simplejson
```

#### Vim

```bash
# jump to previous next cursor position
ctrl + o         # previous cursor position
ctrl + i         # next
:tabn [Tab No.]  # go to [Tab No.] tab
,tn              # open a new tab
ctrl + f         # find a file under this directory
```



### 02/28

---

#### thesis related

- `IWSLT17 FR-EN` baseline is out, valid best BLEU: **39.6**

- ~~Implement ST, BT, TA training.~~

  - *Walk through* `generate.py` or other files to learn how to decode a file and restore its instance order.

    > - ST is to use the baseline/forward model to generate train target.
    > - BT is to use the backward model to generate train source.
    > - TA is to use the forward right-to-left model to generate train target.
    >   - How to train with right-to-left order? e.g. change the batch format in train_step?

- Run `IWSLT17 EN-FR` baseline (2 A100): valid best BLEU: **40.33**

- Run `WMT20 ZH-EN` baseline (2 A100)

  - dropout 0.15, warmup-step: 4000, lr: 5e-4 $$\Rightarrow$$ valid best BLEU: 15.95
  - dropout 0.25, warmup-step: 4000, lr: 5e-4 $$\Rightarrow$$ valid best BLEU: **16.24** (epoch 30+)

---

#### A bag of questions about details in TOD papers

- In simpleTOD, what is the initialization of the autoregressive decoder-only model? Is that initialization the same as that of SOLOIST?
- Does simpleTOD include dialogue acts in the sequence of autoregressive training tokens? How about SOLOIST?
- Figure out the difference between end-to-end evaluation results and context-to-response evaluation.
  - They both can be evaluated w.r.t. Inform, Success, BLEU, Combined. 

- What is the concept of `session` mean? What are the differences between `session` and dialogue?

- **[IMPORTANT]** How to evaluate without dialogue states? read the `evaluate.py` script [here](https://github.com/budzianowski/multiwoz/blob/master/evaluate.py).



### 03/01

---

#### thesis related

- Training â€‹[:heavy_check_mark: without random shuffle]

  - Run `WMT20 EN-ZH` baseline (2 A100), using same hyperparameter as `ZH-EN` direction.
  - Run `WMT14 EN-DE` and `DE-EN` baseline (2 A100), using the old hyperparameters.

- Testing [:x:]

  - `EN-FR`, `FR-EN`, detokenized (via `SacreBLEU==1.5.1`) BERTScore with respect to reference.
  - Same to `EN-DE` and `ZH-EN`.

  > **[IMPORTANT]** Write scripts to extract Detokenized reference and hypothesis text from output of `generate.py`.

---

#### Answer [A bag of questions about details in TOD papers](#a-bag-of-questions-about-details-in-tod-papers)



#### Walk through evaluation in Multiwoz (DST and E2E Dialogue Generation)

> Re-run simpleTOD/UBAR and Zero-Shot DST.



#### Learning the differences between policy optimization and end-to-end models



### 03/02

Talk with Haoning, Junwei, Haipeng, Yifan about **error propagation** issue in DST.



### 03/03

---

#### thesis related

- [:heavy_check_mark:] run `right-to-left` model on `ZH-EN` `EN-FR`/`FR-EN`.
- [:heavy_check_mark:] run `right-to-left` model on `EN-DE`.
- ~~Test baseline BERTScore on the 4 translation tasks.~~
- ~~Run generation on train set for `ST` and `BT` training.~~

---

```bash
no model response
test Corpus Matches : 22.10%
test Corpus Success : 2.50%
test Corpus BLEU : 1.0000%
Total number of dialogues: 1000
```



### 03/04

---

- ~~Test baseline BERTScore on the 4 translation tasks.~~
- ~~Run generation on train set for `ST` and `BT` training.~~

---

