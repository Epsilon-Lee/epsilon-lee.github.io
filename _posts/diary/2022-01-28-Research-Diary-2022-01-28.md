---
layout: post
title: "Research Diary from 2022/01/28"
author: Guanlin Li
tag: diary
---

- toc
{:toc}
### 01/28

#### Is interpretable ML (IML) overpromising and underdelivering? `interpretale ml`

- An [article]() from ACM Queue about **current problems** of Interpretable ML (IML)
- `TL;DR` we describe some of the **obstacles** impeding progress in this field and propose a <u>'diagnostic'</u> vision for IML to overcome these obstacles.
  - **Problem**: a lack of clear evidence and usage guidelines for how IML (e.g. better trust and debugging) may actually help consumers.
  - **Solution**: researchers and consumers should work together to identify well-defined *target use cases* as part of developing a diagnostic treatment of IML methods.



#### Discussion with Yang Zhao about domain identification and clustering `application` `data-centric` `domain adaptation`

- [Unsupervised Domain Clusters in Pretrained Language Models](https://arxiv.org/pdf/2004.02105.pdf), ACL 2020.

We read the above paper and learn about `GMM` clustering of dense sentence vectors, **purity** metrics for unsupervised evaluation of clustering.



#### What is `webdataset`? How about other toolbox for prototyping and creating DL projects? `toolkit`

> I would like to use the toolkits here to build research projects or toy applications in my spare time.

- [webdataset](https://github.com/webdataset), A high-performance Python-based I/O system for large (and small) deep learning problems, with strong support for PyTorch.
- [functorch](https://github.com/pytorch/functorch), functorch is a prototype of JAX-like composable function transforms for PyTorch.
- [python poetry](https://python-poetry.org/), `python` environment management.



### 02/01

#### How to get data resources for pre-training? `data-centric` `engineering`

- [Documenting Geographically and Contextually Diverse Data Sources: The BigScience Catalogue of Language Data and Resources](https://arxiv.org/pdf/2201.10066.pdf), Jan. 25 2022.

Jiaxing mentioned to me the following data resources for Chinese:

- [CLUECorpus2020](https://arxiv.org/pdf/2003.01355.pdf), [github](https://github.com/CLUEbenchmark/CLUECorpus2020/)
- [wudao](https://github.com/TsinghuaAI/CPM) 200G(?)

---



### 02/06

#### OOD Literature review

- [Generalized Out-of-Distribution Detection: A Survey](), Oct. 21 2021.
  - Closed-word assumption vs open-world assumption
  - Distributional shift: 
    - The detection of **semantic shift** is the focal point in OOD detection task considered in this paper, where the **label space** can be different between ID and OOD data, and hence the model should not make any prediction.
  - A survey of generalized out-of-distribution detection (five sub-topics), namely OD, OOD, AD, ND (D=detection), OSR (Open Set Recognition)



### 02/08

#### 100 `numpy` exercise

##### Basics

```python
# 1. Import the numpy package under the name np
import numpy as np

# 2. Print the numpy version and the configuration
print(np.__version__)
print(np.show_config())

# 3. Create a null vector of size 10
np.zeros(10) # (10,)

# 4. How to find the memory size of any array
Z = np.zeros((10,10))
print("%d bytes" % (Z.size * Z.itemsize))

# 5. How to get the documentation of the numpy add function from the command line?
%run `python -c "import numpy; numpy.info(numpy.add)"`

# 6. Create a null vector of size 10 but the fifth value which is 1

# 7. Create a vector with values ranging from 10 to 49
np.arange(10, 50)

# 8. Reverse a vector (first element becomes last)
Z = np.arange(50)
Z = Z[::-1]
print(Z)

# 9. Create a 3x3 matrix with values ranging from 0 to 8

# 10. Find indices of non-zero elements from [1,2,0,0,4,0]
nz = np.nonzero([1,2,0,0,4,0])
print(nz)

Z = np.arange(9).reshape(3, 3)
nz = np.nonzero(Z)  # two arrays of x index and y index.

# 11. Create a 3x3 identity matrix
np.eye(3)

# 12. Create a 3x3x3 array with random values
np.random.random((3, 3, 3))

# 13. Create a 10x10 array with random values and find the minimum and maximum values
Z = np.random.random((10, 10))
Zmin, Zmax = np.min(Z), np.max(Z)

# 14. Create a random vector of size 30 and find the mean value
z = np.random.random((30))  # np.random.random(30)
zMean = np.mean(z)

# 15. Create a 2d array with 1 on the border and 0 inside
Z = np.zeros((10,  10))
Z[0, :] = 1
Z[-1, :] = 1
Z[:, 0] = 1
Z[:, -1] = 1  # four assignment expression

Z = np.ones((10, 10))
Z[1:-1, 1:-1] = 0

# 16. How to add a border (filled with 0's) around an existing array?
Z = np.ones((5, 5, 5))
ZPadded = np.pad(
  Z,
  pad_width=1,
  mode='constant',
  constant_values=0
)
## How to understand padding operation in high dimensional space?

# 17. What is the result of the following expression?
0 * np.nan               --> nan
np.nan == np.nan         --> False
np.inf > np.nan          --> False
np.nan - np.nan          --> nan
np.nan in set([np.nan])  --> True
0.3 == 3 * 0.1           --> False

# 18. Create a 5x5 matrix with values 1,2,3,4 just below the diagonal

# 19. Create a 8x8 matrix and fill it with a checkerboard pattern

# 20. Consider a (6,7,8) shape array, what is the index (x,y,z) of the 100th element?
np.unravel_index(99, (6, 7, 8))
```



### 02/16, 02/17

#### About autoregressive DPR

1. Read and summarize the papers
2. Discuss motivations, current issues, and room for improvement



### 02/18

#### Group meeting log

王广涛：self-supervised graph representation learning

- 任务复杂，监督信号特别少
  - Node classification: Pubmmed数据集，仅仅60个数据点被标注
  - Link prediction/KG completion（sparse graph, e.g. ATOMIC <= 3 degree，304，388 entities）
- SSL方法：
  - Mask LM（Node mask, Edge mask）
  - Contrastive learning
  - Bootstrap Your Own Latent：不要contrastive pair
- contrastive vs non-contrastive --> collapse
- 框架的三块：
  - Graph augmentation
    - Perturbed graph (rule-based, heuristic): 对于sparse的graph，随即删除对影响很大的，node上无feature
    - Preserve inherent properties of the graph
      - Multi-hop edge addition：加边不影响连通性，加边后，拉普拉斯半径不变
    - **Graph generation evaluation**: e.g. MMD
      - Clustering coefficient: 聚类系数为什么可以做
      - Degree distribution
      - Laplacian spectrum
  - Graph encoder
    - Subgraph sampling?
      - Delta-theory
      - Deep GNN + shallow sampler: the hop of the sub-graph should be smaller than the layers of GNN
    - Relative node position: structure一样也有区别
    - Supernode: similar to `CLS`
    - Multi-hop attention GNN
  - Loss

> Qs:
>
> 1. 图片容易定义negative，但是graph中的node如何定义negative呢？
> 2. CL要足够大么？batch size如何？
> 3. graph augmentation技术？
> 4. 一个训练样例图的规模？
> 5. 聚类系数为什么可以做graph generation evaluation？
> 6. 除了subgraph的表示方式？还有别的什么方式么？为什么不合适？



### DPR codebase code structure `coding`

- News:
  - [HuggingFace Adds DPR model](https://github.com/huggingface/transformers/pull/5279).

