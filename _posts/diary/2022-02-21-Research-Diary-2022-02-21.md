---
layout: post
title: "Research Diary from 2022/02/21"
author: Guanlin Li
tag: diary
---

- toc
{:toc}


### 02/21

---

#### thesis related

1. Learn how to use `BERTScore`, and run an experiment on two corpus.

> ```bash
> # before call this function, all the text files should be detokenized!
> 
> bert-score -r example/refs.txt -c example/hyps.txt --lang en
> bert-score -r example/refs.txt -c example/hyps.txt --lang en --rescale_with_baseline
> 
> # multiple reference
> bert-score -r example/refs.txt example/refs2.txt -c example/hyps.txt --lang en
> ```

1. Download and preprocess `IWSLT17 EN-FR`, `WMT14 EN-DE` datasets, check where to download `WMT EN-ZH` datasets.
2. ~~Run `transformer` baseline on `IW` dataset.~~ `Use Yongwei's GPU machine, thanks Yongwei`

---

#### What are the related works to `LaMDA` and `Meena`?



#### ~~How to learn (few-shot, zero-shot) task-oriented dialogue system?~~

> HelloGPT2
>
> Contrastive learning

#### ~~What benchmarks to use? The data form of each datasets?~~



### 02/22

---

#### thesis related

- [ ] Ask Yongwei for help: use his GPU machine and config. running environment and start run experiments on `WMT14 EN-DE`.

- [ ] Read the `fairseq-da` code, make sure the training output **clear** log.

- [ ] Preprocess `WMT20 ZH-EN` file to get 1000,000 bitext pairs.

---

#### What benchmarks to use?  What are their data formats?

> Read `MultiWoz`, `TicketTalk`

#### What are the current experimental settings for few-shot TOD? What are the main techniques?

> Grasp some intuitions of this question by reading `TOD-BERT` and `SOLOIST`.

#### What are the core problems in few-shot TOD?






