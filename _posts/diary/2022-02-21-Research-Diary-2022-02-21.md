---
layout: post
title: "Research Diary from 2022/02/21"
author: Guanlin Li
tag: diary
---

- toc
{:toc}


### 02/21

---

#### thesis related

1. Learn how to use `BERTScore`, and run an experiment on two corpus.

> ```bash
> # before call this function, all the text files should be detokenized!
> 
> bert-score -r example/refs.txt -c example/hyps.txt --lang en
> bert-score -r example/refs.txt -c example/hyps.txt --lang en --rescale_with_baseline
> 
> # multiple reference
> bert-score -r example/refs.txt example/refs2.txt -c example/hyps.txt --lang en
> ```

1. Download and preprocess `IWSLT17 EN-FR`, `WMT14 EN-DE` datasets, check where to download `WMT EN-ZH` datasets.
2. ~~Run `transformer` baseline on `IW` dataset.~~ `Use Yongwei's GPU machine, thanks Yongwei`

---

#### What are the related works to `LaMDA` and `Meena`?

> **[Done]**: mainly about evaluation (learnable metrics)

#### ~~How to learn (few-shot, zero-shot) task-oriented dialogue system?~~

> HelloGPT2
>
> Contrastive learning

#### ~~What benchmarks to use? The data form of each datasets?~~



### 02/22

---

#### thesis related

- Ask `Yongwei` for help: use his GPU machine and config. running environment and start run experiments on `WMT14 EN-DE`. **[Done]**

- Read the `fairseq-da` code, make sure the training output **clear** log. **[Unfinished]**

- Preprocess `WMT20 ZH-EN` file to get 1000,000 bitext pairs. **[Unfinished]**

---

#### What benchmarks to use?  What are their data formats?

> Read `MultiWoz`, `TicketTalk`, `Ubuntu dialogue`

- `MultiWoz`: Each dialogue is annotated with a sequence of dialogue states and corresponding system dialogue acts, hence, MultiWOZ can be used to develop individual system modules as separate classification tasks and serve as a benchmark for existing modular-based approaches.
  - Existing dialogue datasets can be divided into: machine-machine, human-machine, human-human

#### ~~What are the current experimental settings for few-shot TOD? What are the main techniques?~~

> Grasp some intuitions of this question by reading `TOD-BERT` and `SOLOIST`.

#### ~~What are the core problems in few-shot TOD?~~



#### Reading this [corpus survey](https://arxiv.org/abs/1512.05742) and extract corpora for TOD

This survey has **6** sections:

1. Intro

2. Characteristics of data-driven dialogue system: ***non-goal oriented, goal driven; component-based, end-to-end***.

   ![](../../public/img/diary/dialogue_system/dialogue_system.png)

3. Dialogue interaction types & aspects

   - Written, spoken, multi-modal

   - Human-human, human-machine

     ![](../../public/img/diary/dialogue_system/human_human_vs_human_machine.png)

   - Corpus size

   - Naturalness

4. Available datasets

   - there are five tables [here](https://breakend.github.io/DialogDatasets/)

5. Discussion

6. Conclusion

> Section 4 provides categorisation of all kinds of dialogue corpora according to principles proposed in Section 2 and 3.
>
> **Practice**
>
> I learn how to download OpenTiitle corpus espectially for a single languange.



### 02/23

---

#### thesis related

- Read the `fairseq-da` code, to learn the training `options`, especially for **logging**.
- Prepare `WMT20 ZH-EN` 1,000,000 bitext.
- Prepare `IWSLT17 EN-FR` bitext.

---

#### Find and summarize benchmarks for TOD in recently years; record them in `overleaf`.

> Including recent multimodal TOD benchmarks.

#### What are the current experimental settings for few-shot TOD? What are the main techniques?

> `SOLOIST` and `TOD-BERT`

- TOD-BERT:
  - **representation learning**, **discriminative** **energy-model**
  - "we unify nine human-human and multi-turn task-oriented dialogue datasets for language modeling"
  - "we incorporate user and system tokens into the masked language modeling"
  - "contrastive objective function to simulate the response selection task"
- SOLOIST:
  - **generative**

#### ~~Summarize recent years' TOD works, summarize the trends into `Keynote`.~~

> EMNLP 2021
>
> ACL 2021
>
> NAACL 2021

- Reading surveys *[Recent Advances and Challenges in Task-oriented Dialog Systems](https://arxiv.org/pdf/2003.07490.pdf)*, Jun. 23 2020 and [A Survey on Dialogue Systems: Recent Advances and New Frontiers](https://www.kdd.org/exploration_files/19-2-Article3.pdf), `kdd` 2019.
  - "Task-oriented systems aim to assist the user to complete certain task (e.g. finding products, booking accommodation and resterauts)"



### 02/24

---

#### thesis related

- Finish yesterday's plan

---

#### ~~What are the challenges in the above two surveys for TOD?~~



#### ~~Summarize 21 *CL venues' TOD-related papers in another `.md` file with `TL;DR`.~~



#### Huaishao's talk

> Vision knowledge to help language understanding.

**Background**

Multimodal pretraining can not help language understanding much. WHY?

- Vokenization: what are the downstream tasks?
- VisAD
- UVR-NMT
- CLIP
- vision grounding assists NLU?

### 02/25

---

#### thesis related

- ~~Finish yesterday's plan~~

---

#### Prepare Job Statement Talk.



#### What are the challenges in the above two surveys for TOD? 



#### `talk` Self-explaining model



### 02/26

---

#### thesis related

- Finish 02/24's plan
- `fairseq-da` options

```python
# max_updates
```



---

### 02/27

- Conversational AI toolkits
  - [Rasa](https://github.com/RasaHQ/rasa), Open source machine learning framework to automate text- and voice-based conversations: NLU, dialogue management, connect to Slack, Facebook, and more - Create chatbots and voice assistants.
  - [Cornell Conversational Analysis Toolkit](https://github.com/CornellNLP/Cornell-Conversational-Analysis-Toolkit), ConvoKit is a toolkit for extracting conversational features and analyzing social phenomena in conversations. It includes several large conversational datasets along with scripts exemplifying the use of the toolkit on these datasets.
  - [ConvLab-2](https://github.com/thu-coai/ConvLab-2), ConvLab-2: An Open-Source Toolkit for Building, Evaluating, and Diagnosing Dialogue Systems.
  - [LAUG](https://github.com/thu-coai/LAUG), Language Understanding Augmentation Toolkit for Robustness Testing.



---

---

### Ideas Collection

> Record several sudden ideas that hit me while reading or wandering.

#### `TOD` Large-scale pretraining with weak lingusitic labels for zero-shot TOD

This is motivated from [CS-BERT](https://aclanthology.org/2021.nlp4convai-1.13/) where they use several NER as a downstream task to evaluate the pre-trained model. However, since in task-oriented dialogue, entities are very important, and **entity-aware pretraining** is just to predict the text span (in a binary manner) whether it is an entity.

This can be further extended to **large-scale pretraining with linguistic knowledge**, that is a multitask learning process over dialogue turns $$x = (x_1, x_2, ..., x_n)$$ with sequences of **LINEARIZED** linguistic labels $$z^l = (z^l_1, z^l_2, ..., z^l_n)$$ from several core linguistic tasks, namely, POS tagging, NER, Semantic Role Labeling, Constituent/Dependency Parsing, information/relation extraction from [stanza](https://stanfordnlp.github.io/stanza/) or other off-the-shelf core NLP task toolkits, e.g. [AllenNLP](https://allenai.org/allennlp/software/allennlp-library), [spaCy](https://spacy.io/).

> **Other thought.**
>
> 1. How about using more high-level NLP tasks such as weak **sentiment annotation** and so on?
> 2. How about using augmented sequence training task like in [Structured Prediction as Translation between Augmented Natural Languages](https://arxiv.org/abs/2101.05779).

Can there be **zero-shot** ability of the model?

<u>Some important points</u>:

- **Data-scale**: refer to several works that take advantage of all existing TOD corpora, to see how many dialogues/turns/words can be available in weak label annotation and pretraining.
- **Task-relatedness**: how each linguistic task can be leveraged for the downstream TOD task?

> **Caveat.**
>
> This idea needs more motivations and evidences to support, read [Fine-tuned LMs are zero-shot learner](https://arxiv.org/pdf/2109.01652.pdf).

#### `TOD` Issues in Evaluation for zero-shot TOD



#### `TOD` An Investigation of Few-Shot Learning Techniques for TOD

- [Few-shot Learning With Language Models](https://github.com/tonyzhaozh/few-shot-learning), GitHub repo.
  - Few-shot Learning of GPT-3
- [LibFewShot: A Comprehensive Library for Few-shot Learning.](https://github.com/RL-VIG/LibFewShot), Github repo.
  - I can refer to their implementation of a bunch of few-shot learning algorithms.



#### `ODQA` Learning Dense Retrieval without Contrastive Learning

> Using regularized methods for training DPR, like that talked by LeCun.

