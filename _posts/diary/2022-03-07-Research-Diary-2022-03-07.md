---
layout: post
title: "Research Diary from 2022/03/07"
author: Guanlin Li
tag: diary
---

- toc
{:toc}


### Milestone

#### `BERTScore` table

| Method   | IWSLT FR-EN | IWSLT EN-FR | WMT20 ZH-EN | WMT14 EN-DE |
| -------- | ----------- | ----------- | ----------- | ----------- |
| Baseline | 0.959051    | 0.841695    | 0.897975    | 0.921766    |
| RAML     |             |             |             |             |
| SO       |             |             |             |             |
| ST       |             |             |             |             |
| BT       |             |             |             |             |
| TA       |             |             |             |             |



- ST: src-->tgt
- BT: tgt-->src
- TA: src-->tgt_r2l





### 03/07

---

#### thesis related

- Test with BERTScore: write *detoknized* reference/hypothesis extraction script.
- Generate with only reference/hypo (**no detok!!**).
- Write code for switchout and RAML.

---



### 03/08

---

#### thesis related

- R2L decoding might be problematic!
  - EN-DE r2l decoding results have punctuations like (. ,) at the beginning of the sentence?! **don't know why?**
- 

---

#### Run ZeroShot-DST



#### Write technique summary of untod

- **Formulation**
- **Relation to semantic parsing**: candidate semantic parse generation $$\Rightarrow$$ how to explore from a RL perspective (language understanding), then rank based on all possible features
- **Ontology-based *formal language***: constrained generation with key-value style language

- **Unsupervised DST**
  - Another solution: unsupervised alignment between utterance and **verbalized** version of possible belief states
- How to handle info. redundancy?
  - Information Bottleneck



### 03/10

#### A total thread about [Early Stopping](https://twitter.com/tomgoldsteincs/status/1501326973041266696) trick is educational!



#### Zero-shot slot-filling?

